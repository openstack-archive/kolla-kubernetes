*Bare Metal Deployment Guide for kolla-kubernetes

Follow this guide to setup Kubernetes on bare metal:
http://deploy-preview-1321.kubernetes-io-vnext-staging.netlify.com/docs/getting-started-guides/kubeadm/

Another choice is Kargo for deployment

Here is a video sdake showed at a Meetup on February 15th, 2017:
https://youtu.be/rHCCUP2odd8

Co-Authored-By: rwellum
co-Authored-By: zhubingbing
Co-Authored-By: sambetts
Co-Authored-By: bradjones
Co-Authored-By: sogabe
Co-Authored-By: qwang
Co-Authored-By: duonghq

# TODO(sdake): This file is generated by
#    /opt/kolla-kubernetes/tools/build_example_yaml.py
#    https://etherpad.openstack.org/p/kolla-cloudyaml
# The example should be pruned to just the changes needed to run, rather
# than force all defaults to their default value. Its hard to see what
# actually changed this way.
 
From a working kubeadm deploy of kubernetes on TWO fresh machines (used machines may trigger problems)
watch kubectl get pods --all-namespaces
Wait for all pods including DNS to be running

Dependencies:
docker > 1.12.5
Kubernetes == 1.5.4
Helm == 2.2.2
kubeadm >= 1.5.0, < 1.6.0

(CHERRYPICK) This patch is needed as kollakube is broken as is (ignore gate failures at present - the patch is incomplete but works for this documentation):
https://review.openstack.org/#/c/439740/

sudo yum install -y epel-release
sudo yum install -y ansible
sudo yum install -y python-pip
sudo yum install -y python-devel

# git a clone of kolla-ansible repository for genconfig operation
git clone http://github.com/openstack/kolla-ansible

# git a clone of kolla-kubernetes repository for the implementation
git clone http://github.com/openstack/kolla-kubernetes

# apply a cherrypick that fixes kollakube tool
cd kolla-kubernetes
git fetch ssh://sdake@review.openstack.org:29418/openstack/kolla-kubernetes refs/changes/40/439740/1 && git cherry-pick FETCH_HEAD
cd ..

sudo pip install -U kolla-ansible/ kolla-kubernetes/

# copy defualt kolla configuration to etc
sudo cp -aR /usr/share/kolla-ansible/etc_examples/kolla /etc 

# copy default kolla-kubernetes configuration to /etc
# TODO(sdake) is this correct, or shoudl it be kolla-kubernetes/* /etc/kolla
sudo cp -aR /usr/share/kolla-kubernetes/etc/kolla-kubernetes /etc

# generate default passwords via SPRNG
sudo kolla-genpwd

# Create a k8s namespace to isolate this kolla deployment
kubectl create namespace kolla

# Label the non-master node as the compute and controller node
ALLINONENODE=$(hostname)
kubectl label node $ALLINONENODE kolla_compute=true
kubectl label node $ALLINONENODE kolla_controller=true

# TODO(sdake) sort out how to use kubectl here
# It's possible the hostname is not the same as kubenetes cluster node's
name. If so, `kubectl label node ` won't be able to find the node name.
use `kubectl get nodes` to populate the names of nodes.


Need to edit globals.yml? I.e: http://paste.openstack.org/raw/600976/
	* and edit storage to your liking.
	* Although if you choose Ceph will we need more than two nodes?
	* (afaik ceph only supports single node atm? - pete)

# Generate the default configuration
sudo kolla-ansible genconfig

# Generate the Kubernetes secrets and register them with Kubernetes
sudo kolla-kubernetes/tools/secret-generator.py create

# Create and register the Kolla config maps
kollakube res create configmap  \
    mariadb keystone horizon rabbitmq memcached nova-api nova-conductor  \
    nova-scheduler glance-api-haproxy glance-registry-haproxy glance-api  \
    glance-registry neutron-server neutron-dhcp-agent neutron-l3-agent \
    neutron-metadata-agent neutron-openvswitch-agent openvswitch-db-server \
    openvswitch-vswitchd nova-libvirt nova-compute nova-consoleauth \
    nova-novncproxy nova-novncproxy-haproxy neutron-server-haproxy  \
    nova-api-haproxy cinder-api cinder-api-haproxy cinder-backup  \
    cinder-scheduler cinder-volume iscsid tgtd keepalived

# Enable resolv.conf workaround
sudo kolla-kubernetes/tools/setup-resolv-conf.sh kolla

# Initialize helm and wait for "Running" state for tiller
helm init
watch "kubectl get pods -n kube-system | grep tiller"

# Verify both the client and server version of Helm are consistent
helm version

# Build all helm microcharts, service charts, and metacharts
kolla-kubernetes/tools/helm_build_all.sh .

# Check that all helm images have been built:
ls | grep ".tgz" | wc -l
146

# Link to kolla-service-start.sh: https://etherpad.openstack.org/p/kolla-service-start

# TODO(sdake): sort out what to do with these comments:
# guess just make a decision if deterministic or use /opt 
# Requires cloud.yaml, https://etherpad.openstack.org/p/kolla-cloudyaml or use
# missing a ws before "cinder" in line 50
#./kolla-kubernetes/tools/build_example_yaml.py to build it.
# It's then commented out - user has to decide what to customize.
#  Possibly provide a minimum working example - or the build_example could
# take some user inputs
# Also note two interfaces need to be configured unique to the user
# ext_interface_name and tunnel_interface
# And external bridge needs to be named correctly: ext_bridge_name:
# Also note refers to ./cloud.yaml - but may not be where user has
# generated this file
# Error: failed to parse ./cloud.yaml: error converting YAML to JSON: yaml:
# line 49: did not find expected key
* That's the tunnel_interface - what should this tunnel interface be?
* May be related to external_vip: 10.87.49.247 - will test today and see if unblocked
# The error is caused by missing a whitespace before "cinder" in line 50.
# Mar 14 Cool I will check this out.

# Start all service level charts
helm install --debug /opt/kolla-kubernetes/helm/service/mariadb --namespace kolla --name mariadb --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/rabbitmq --namespace kolla --name rabbitmq --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/memcached --namespace kolla --name memcached --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/keystone --namespace kolla --name keystone --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/glance --namespace kolla --name glance --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/cinder-control --namespace kolla --name cinder-control --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/microservice/cinder-volume-lvm-daemonset --namespace kolla --name cinder-volume --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/horizon --namespace kolla --name horizon --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/openvswitch --namespace kolla --name openvswitch --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/neutron --namespace kolla --name neutron --values ./cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/nova-control --namespace kolla --name nova-control --values /root/cloud.yaml
helm install --debug /opt/kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values /root/cloud.yaml

# wait for all pods to enter running state
watch kubectl get pods -n kolla

# Generate openrc file
sudo kolla-ansible post-deploy

source /etc/kolla/admin-openrc.sh

# Bootstrap the cloud envrionment
./init-runonce

# Create a floating IP address and retrieve the IP address
openstack floating ip create public1

# Add a floating IP to the machine
openstack server add floating ip demo1 {FILL IN IP}

To cleanup the database entry for a specific service such as nova:
helm install --debug /opt/kolla-kubernetes//helm/service/nova-cleanup --namespace kolla --name nova-cleanup --values cloud.yaml

# TROUBLESHOOTING

# MariaDB bootstrap fails
helm delete --purge mariadb

# on controller nodes:
rm -rf /var/lib/kolla/volumes/mariadb/*
